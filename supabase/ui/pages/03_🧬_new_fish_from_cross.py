# 02_ðŸ“¤_new_fish_from_csv.py
from __future__ import annotations

import io, os, re
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import date, datetime, timedelta

import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

# ---------- Auth gate ----------
try:
    from supabase.ui.auth_gate import require_app_unlock
except Exception:
    from auth_gate import require_app_unlock
require_app_unlock()

PAGE_TITLE = "CARP â€” New Fish from CSV"
st.set_page_config(page_title=PAGE_TITLE, page_icon="ðŸ“¤")

# ---------- DB engine ----------
ENGINE: Optional[Engine] = None
def _get_engine() -> Engine:
    global ENGINE
    if ENGINE:
        return ENGINE
    url = os.getenv("DB_URL")
    if not url:
        raise RuntimeError("DB_URL not set")
    ENGINE = create_engine(url, future=True)
    return ENGINE

# ---------- helpers: schema detection ----------
def _table_has(schema: str, table: str) -> bool:
    q = text("""
      select 1 from information_schema.tables
      where table_schema=:s and table_name=:t limit 1
    """)
    with _get_engine().begin() as cx:
        return cx.execute(q, {"s": schema, "t": table}).first() is not None

def _columns_in(schema: str, table: str) -> List[str]:
    q = text("""
      select column_name
      from information_schema.columns
      where table_schema=:s and table_name=:t
      order by ordinal_position
    """)
    with _get_engine().begin() as cx:
        return [r[0] for r in cx.execute(q, {"s": schema, "t": table}).all()]

def _function_exists(schema: str, name: str) -> bool:
    q = text("""
      select 1
      from pg_proc p join pg_namespace n on n.oid=p.pronamespace
      where n.nspname=:s and p.proname=:n
      limit 1
    """)
    with _get_engine().begin() as cx:
        return cx.execute(q, {"s": schema, "n": name}).first() is not None

# ---------- robust date parser ----------
try:
    from dateutil import parser as _p
except Exception:
    _p = None

_RX_ISO = re.compile(r"^\d{4}-\d{2}-\d{2}$")
_RX_LETTER_PREFIX = re.compile(r"^[A-Za-z](.*)$")
_RX_8DIGITS = re.compile(r"^\d{8}$")

def parse_date_birth(x):
    if x is None:
        return None
    if isinstance(x, datetime):
        return x.date()
    if isinstance(x, date):
        return x
    s = str(x).strip()
    if not s:
        return None
    # strip quotes
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ("'", '"'):
        s = s[1:-1].strip()
    # allow dYYYY-MM-DD
    m = _RX_LETTER_PREFIX.match(s)
    if m:
        s = m.group(1).strip()
    # YYYY-MM-DD
    if _RX_ISO.match(s):
        y, m_, d_ = map(int, s.split("-"))
        return date(y, m_, d_)
    # YYYYMMDD
    if _RX_8DIGITS.match(s):
        y, m_, d_ = int(s[0:4]), int(s[4:6]), int(s[6:8])
        try:
            return date(y, m_, d_)
        except Exception:
            pass
    # Excel serials (1899 or 1904 base)
    try:
        n = float(s)
        if 1 <= n < 100000:
            d1 = date(1899, 12, 30) + timedelta(days=int(n))
            if 1900 <= d1.year <= 2100:
                return d1
            d2 = date(1904, 1, 1) + timedelta(days=int(n))
            if 1900 <= d2.year <= 2100:
                return d2
    except Exception:
        pass
    # free-form
    if _p is not None:
        try:
            return _p.parse(s, fuzzy=True, dayfirst=False, yearfirst=False).date()
        except Exception:
            return None
    return None

# ---------- UI ----------
st.title(PAGE_TITLE)
st.caption("Upload CSV with: name, date_birth, genetic_background. fish_code is generated by the DB. Optional link columns: transgene_base_code, allele_nickname, zygosity.")

uploaded = st.file_uploader("CSV file", type=["csv"])
if not uploaded:
    st.info("Choose a CSV to preview.")
    st.stop()

# Seed batch ID only (no batch label)
default_batch = Path(getattr(uploaded, "name", "")).stem
seed_batch_id = st.text_input("Seed batch ID", value=default_batch)

# Load CSV (preview + normalized)
try:
    df_raw = pd.read_csv(uploaded)
    df = pd.read_csv(io.BytesIO(uploaded.getvalue()),
                     converters={"date_birth": parse_date_birth})
except Exception as e:
    st.error(f"Failed to read CSV: {e}")
    st.stop()

# Ignore incoming fish_code â€” DB generates it
if "fish_code" in df.columns:
    df = df.drop(columns=["fish_code"])

# Required minimum
REQUIRED = ["name", "date_birth", "genetic_background"]
for col in REQUIRED:
    if col not in df.columns:
        df[col] = None

# Resolve link columns
ALIASES = {
    "transgene_base_code": ["transgene_base_code", "base_code", "tg_base_code", "transgene_base", "tg_base"],
    "allele_nickname":     ["allele_nickname", "allele_nick", "allele_name", "allele"],
    "zygosity":            ["zygosity", "zyg", "allele_zygosity"],
}
def _resolve(df: pd.DataFrame, want: str) -> Optional[str]:
    for cand in ALIASES.get(want, []):
        if cand in df.columns:
            return cand
    return None

col_tg   = _resolve(df, "transgene_base_code")
col_nick = _resolve(df, "allele_nickname")
col_zyg  = _resolve(df, "zygosity")

# Make dates readable for preview
if "date_birth" in df.columns:
    df["date_birth"] = df["date_birth"].apply(lambda d: d.isoformat() if isinstance(d, date) else ("" if pd.isna(d) else str(d)))

# Tabs
tab_raw, tab_norm, tab_cols = st.tabs(["Raw CSV", "Normalized", "Columns"])
with tab_raw:
    st.dataframe(df_raw.head(200), width="stretch", hide_index=True)
with tab_norm:
    candidate_cols = [
        "name", "date_birth", "genetic_background", "nickname",
        "line_building_stage", "description", "created_by", "notes",
        col_tg, col_nick, col_zyg
    ]
    cols = [c for c in candidate_cols if c and c in df.columns]
    st.dataframe(df[cols].head(500) if cols else df.head(500), width="stretch", hide_index=True)
with tab_cols:
    missing = [c for c in REQUIRED if c not in df.columns or df[c].isna().all()]
    ok = [c for c in REQUIRED if c in df.columns and not df[c].isna().all()]
    st.write("**Required fields**")
    if ok: st.success(", ".join(ok))
    if missing: st.error("Missing or empty: " + ", ".join(missing))
    st.write("**Detected link columns**")
    st.info(
        f"transgene_base_code â†’ {col_tg or 'â€”'}, "
        f"allele_nickname â†’ {col_nick or 'â€”'}, "
        f"zygosity â†’ {col_zyg or 'â€”'} "
        "(allele_number is generated by the database)"
    )

st.divider()

ready = all(c in df.columns and not df[c].isna().all() for c in ["name","genetic_background"]) and "date_birth" in df.columns
if not ready:
    st.warning("Add the missing fields to enable import.")
    st.stop()

st.info("DB will auto-generate **fish_code** for each row; any CSV value is ignored.")

# ---------- Import ----------
if st.button("Import"):
    try:
        tmp = pd.read_csv(io.BytesIO(uploaded.getvalue()),
                          converters={"date_birth": parse_date_birth})
        if "fish_code" in tmp.columns:
            tmp = tmp.drop(columns=["fish_code"])

        # Insert only columns that exist in public.fish
        fish_cols_in_db = set(_columns_in("public", "fish"))
        optional_cols = ["nickname", "line_building_stage", "description", "notes", "created_by"]
        insertable_cols = ["name","date_birth","genetic_background"] + [c for c in optional_cols if c in fish_cols_in_db]

        inserted: List[Dict[str, Any]] = []
        link_rows: List[Dict[str, Any]] = []
        id_by_code: Dict[str, str] = {}

        fish_stmt = text(f"""
            insert into public.fish({", ".join(insertable_cols)})
            values ({", ".join([f":{c}" for c in insertable_cols])})
            returning fish_code, id_uuid, name, date_birth, genetic_background, created_at
        """)

        # NOTE: ensure_transgene_allele( base_code, allele_nickname ) â€” keep your current 2-arg call
        ensure_fn = text("""
            select
              ret_allele_number   as allele_number,
              ret_allele_nickname as allele_nickname
            from public.ensure_transgene_allele(:transgene_base_code, :allele_nickname)
        """)

        # Use direct fish_id (no subselect) for both mapping and linking
        map_stmt = text("""
            insert into public.fish_seed_batches_map(fish_id, seed_batch_id)
            values (:fish_id, :seed_batch_id)
        """)
        link_stmt = text("""
            insert into public.fish_transgene_alleles(fish_id, transgene_base_code, allele_number, zygosity)
            values (:fish_id, :transgene_base_code, :allele_number, :zygosity)
            on conflict do nothing
        """)

        skipped = 0

        # One transaction for everything: insert fish, record batch, ensure alleles, link alleles
        with _get_engine().begin() as cx:
            for _, r in tmp.iterrows():
                fish_row = {k: r.get(k) for k in insertable_cols}
                ins = cx.execute(fish_stmt, fish_row).mappings().one()
                inserted.append(dict(ins))
                id_by_code[ins["fish_code"]] = ins["id_uuid"]

                # Record batch mapping if table exists
                if seed_batch_id and _table_has("public","fish_seed_batches_map"):
                    cx.execute(map_stmt, {"fish_id": ins["id_uuid"], "seed_batch_id": seed_batch_id})

                # Prepare a potential allele link row (weâ€™ll resolve fish_id from id_by_code)
                tg  = (str(r.get(col_tg)).strip()   if col_tg   and pd.notna(r.get(col_tg))   else "")
                nn  = (str(r.get(col_nick)).strip() if col_nick and pd.notna(r.get(col_nick)) else "")
                zy  = (str(r.get(col_zyg)).strip()  if col_zyg  and pd.notna(r.get(col_zyg))  else "")
                if tg:
                    link_rows.append({
                        "fish_code": ins["fish_code"],
                        "transgene_base_code": tg,
                        "allele_nickname": nn,
                        "zygosity": zy,
                    })

            # Link alleles if function and table exist
            if link_rows and _function_exists("public","ensure_transgene_allele") and _table_has("public","fish_transgene_alleles"):
                for lr in link_rows:
                    got = cx.execute(ensure_fn, {
                        "transgene_base_code": lr["transgene_base_code"],
                        "allele_nickname":     lr.get("allele_nickname") or ''
                    }).mappings().one_or_none()

                    if not got:
                        skipped += 1
                        continue

                    fish_code = lr["fish_code"]
                    fish_id = id_by_code.get(fish_code)
                    if not fish_id:
                        skipped += 1
                        continue

                    cx.execute(link_stmt, {
                        "fish_id":             fish_id,
                        "transgene_base_code": lr["transgene_base_code"],
                        "allele_number":       int(got["allele_number"]),
                        "zygosity":            lr.get("zygosity","")
                    })
            elif link_rows:
                st.warning("Allele link function/table not found; skipped linking.")

        # Display imported rows + genotype summary
        fish_codes = [r["fish_code"] for r in inserted]
        if fish_codes:
            q = text("""
                with links as (
                  select f.fish_code,
                         array_agg(distinct l.transgene_base_code)::text[] as transgene_base_codes,
                         array_agg(l.allele_number)::int[]                 as allele_numbers,
                         array_agg(nullif(l.zygosity,''))::text[]          as zygosities
                  from public.fish f
                  left join public.fish_transgene_alleles l on l.fish_id = f.id_uuid
                  where f.fish_code = any(:codes)
                  group by 1
                )
                select f.fish_code, f.name, f.date_birth, f.genetic_background, f.created_at,
                       coalesce(l.transgene_base_codes, '{}') as transgene_base_codes,
                       coalesce(l.allele_numbers, '{}')       as allele_numbers,
                       coalesce(l.zygosities, '{}')            as zygosities,
                       array_to_string(
                         array(
                           select (l2.transgene_base_code || '^' || l2.allele_number::text)
                           from public.fish_transgene_alleles l2
                           where l2.fish_id = f.id_uuid
                           order by l2.transgene_base_code, l2.allele_number
                         ),
                         '; '
                       ) as genotype_text
                from public.fish f
                left join links l on l.fish_code = f.fish_code
                where f.fish_code = any(:codes)
                order by f.created_at, f.fish_code
            """)
            with _get_engine().begin() as cx:
                show_df = pd.read_sql(q, cx, params={"codes": fish_codes})
        else:
            show_df = pd.DataFrame(inserted)

        st.success(f"Imported {len(inserted)} rows. The database assigned fish_code values.")
        st.subheader("Imported rows")
        if "date_birth" in show_df.columns:
            show_df["date_birth"] = show_df["date_birth"].astype(str)
        st.dataframe(
            show_df[[
                "fish_code","name","date_birth","genetic_background",
                "transgene_base_codes","allele_numbers","zygosities","genotype_text","created_at"
            ]],
            width="stretch", hide_index=True
        )

        if skipped:
            st.warning(f"Allele linking skipped for {skipped} row(s).")

    except Exception as e:
        st.error(f"Import failed: {e}")

else:
    st.stop()